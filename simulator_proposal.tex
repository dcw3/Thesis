\documentclass{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\EX}{\mathbb{E}}
\DeclareMathOperator{\RE}{\mathbb{RE}}
\newcommand{\ita}{\textit}
\newcommand{\eps}{\epsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{\mathbb{P}}

\title{MDP Simulator Proposal}
\author{dcw3}
\date{December 2018}

\begin{document}

\maketitle

\section{Summary of Previous Work}

Overall, I did not find very similar previous work or similar papers. Below are the most relevant previous work I saw:

\subsection{DeepMind Lab}

DeepMind Lab is a 3D RL environment. The user can specify (or programmatically generate) levels, similar to how our simulator will allow users to specify the underlying MDP. The main difference is that DeepMind Lab focuses on a single kind of task (3D navigation), which is fairly complex.
Further research on how level scripts are specified might be useful as inspiration for how we could allow users to programatically specify MDP's.

\subsection{OpenAI Gym}

OpenAI Gym offers a much wider set of tasks than DeepMind Lab, from simple text games to Atari games to robotics tasks. However, most tasks do not offer customization of the environment, and no task allows an arbitrary MDP.

\subsection{Sundry MDP Simulators}

I found two HTML MDP simulators online but they were not particularly useful for this project. One of them was only a Gridworld, and the other only allowed a constant reward for each state, not an arbitrary reward function.

\subsection{TRFL}
Tensorflow supposedly has good RL tools made by Deepmind, in a package called TRFL. I need to look into how to use it, I think.

\subsection{Main Concerns Moving Forward}
How to specify transition matrices and reward functions?

How to write agents? Should I learn Tensorflow? It might be a bad idea to implement algorithms from scratch: supposedly is error-prone.

How to specify priors for agent?

\section{Pseudocode of MDP Simulator}

The pseudocode I have in mind for the MDP simulator is in simulator\_pseudocode.py in this repository.

\section{Defining a Random MDP}
The user specifies $a$, the number of actions, and $m$, the number of states. We then create $a\cdot m$ i.i.d. vectors of length $m$, each of which represents a vector of state-action transition probabilities.

There are two options for choosing these vectors. One is to specify a set number of zero-values for each transition vector, and then choose the remaining values uniformly or logistically. Another option is to choose $m$ values $a_1, \ldots, a_m$ from a standard normal distribution, and then allow the user to specify a cutoff value $\alpha$. Values $a_i$ that are below $\alpha$ are set to $-\infty$. Then, (if any values are remaining) the remaining transition probabilities are set proportional to the value $e^{a_i \beta}$ where $\beta$ is a user-chosen parameter, $\beta\in[0,\infty)$. Higher values of $\beta$ and higher values of $\alpha$ lead to a lower-entropy (more predictable) MDP.

To choose reward functions, the user can specify $m$ values that act as means for normal distributions. The user could also provide $m$ functions as reward functions. We should also provide the option for the user to ask the program to automatically choose reward functions, which could be normal distributions with normally-distributed means.

The starting state will be state $1$ by default.

\section{MDP Generation Process}

Generally speaking, we can view generating an MDP as an iterative process. For now, let us consider a common setting where there is no reward, except for two terminal states, one of which is high-reward and one of which is low-reward.

First, we choose a state $s$ whose transition probabilities are not yet defined. Then, for each action $a$, we choose the states that are reachable from $s$ in one step under action $a$. Then, from within those destination states, we define a transition probability vector with desired entropy. Then, having chosen the transition probability vectors for all actions from $s$, we choose a new state $s$ and begin again.

There is a lot of vagueness and flexibility in this process. The simplest part is choosing a transition probability vector once we have chosen the destination states. If there are $k$ destination states, we use a $k$-dimensional symmetric Dirichlet distribution function, with the concentration parameter $\alpha$. Higher values of $\alpha$ correspond to higher entropy, and this can be specified from the user. For example, for any state-action pair, $\alpha$ could be drawn from a normal distribution with a certain mean.

The next state to choose probabilities for could be chosen from any of the destination states.

The main interesting part would be choosing the destination states of each state-action pair. There are countless properties users might be interested in testing here, so it is hard to account for all possible MDP's the user may wish to create. However, the following are a few properties that come to mind:

\begin{itemize}
  \item How well-structured is the MDP? At the one extreme, all actions from a given state could have the same destination states, just with different probabilities. At the opposite extreme, the destination states of different actions could be completely independent though identically distributed (i.i.d.). The simplest way to specify this would be to specify a distribution for $n_s$ the number of possible destination states, as well as a distribution on $k_{s,a}$ the number of destination states each state-action pair has. Then, each state-action pair's destination states would be uniformly chosen from the $\binom{n_s}{k_{s,a}}$ possible sets of destination states.
  \item How frequently do the states feed into the terminal states? The simplest way to specify this would be to specify some parameter $p$ that grew as the number of states we have yet to specify transition probability vectors for dwindles. $p$ could be the probability that a state is connected to a terminal state.
  \item How recurrent is the MDP? This could be specified as a fraction, $f$, of the destination states that feed back to a state whose transition probability vectors we have already defined.
\end{itemize}
However, the parameter $f$ interacts with the parameter $p$, and I'm not 100\% clear on how that should work. For example, one possibility is that we could introduce a time variable $t$ as we generate the MDP, where $t$ is the number of states whose transition vectors are already determined (i.e. the number of iterations our generation process has done).

\section{Finding a Dirichlet Distribution with Desired Entropy}
Call $Dir_k(\beta)$ a symmetric Dirichlet distribution with parameter $\beta$, of dimension $k$. Then, if $X$ is a random vector of length $k$, and $X\sim Dir_k(\beta)$, then the expected entropy of $X$ is:
$$\EX[H[X]] = \psi(k\beta + 1) - \psi(\beta + 1)$$
Then, $\lim_{\beta\to0} \EX[H[X]] = 0$, as desired. I did not prove that $\lim_{\beta\to\infty} \EX[H[X]] = \log k$, though it makes sense and appears to hold for several values of $k$ when I did some simple computations.

Thus, if a user specified a desired amount of entropy, we could vary the value of $\beta$ to accommodate it. However, given a desired expected entropy $h'$, I am not sure how to find $\beta$ such that $\EX[H[X]] = h'$ where $X\sim Dir_k(\beta)$. In particular, this is because I do not know how to find the inverse of the expression we obtained for $\EX[H[X]]$. The best that comes to mind is some numerical method which takes advantage of the fact that $\EX[H[X]]$ appears to be monotonic in $\beta$.

\section{Correlating Dirichlet Distributions: Option 1}
I did not entirely understand how to obtain the desired correlation between Dirichlet distributions. The best I had was the following:

The mean of a Dirichlet distribution with parameters $\alpha_1, \alpha_2,\ldots,\alpha_k$ is $[\alpha_1/\alpha_0, \ldots,\alpha_k/\alpha_0]$. Then, for a given vector $v$, we can make a particular neighbor $w$ similar to $v$ by drawing $w$ from $Dir(v_1, \ldots, v_k)$, i.e. using a Dirichlet distribution with parameters equal to entries of $v$.

If $X\sim Dir_k(\beta)$,
$$Var(X_i) = \frac{\alpha_i(\alpha_0 - \alpha_i)}{\alpha_0^2(\alpha_0 + 1)}$$
Then, multiplying all $\alpha$ by a factor $k$ will approximately multiply the variance of $X_i$ by $1/k^2$. Then, to increase the "strength" of the correlation, we could draw $w$ from $Dir(kv_1, \ldots, kv_k)$ for some $k > 1$, and we could similarly decrease the strength of correlation by choosing $k < 1$.

If there are multiple neighbors $\{v^1, v^2, \ldots\}$, then we could use their average, vall is $\overline{v}$.

In addition, rather than setting the parameters directly to the average of $\overline{v}$, we could set them to a mixture of $\overline{v}$ and a uniform vector $\beta$ (which represents the prior distribution). For example, we could set them to be equal to $r\overline{v} + (1-r)\beta$.

A substantial concern I have is, how to manage the two parameters. In particular, our answer is:
$$k\bigg(r\overline{v} + (1-r)\beta\bigg)$$
where $k$ influences variance and $r$ influences the mean. It's not obvious to me how $k$ and $r$ interact to calculate the actual correlation.

\section{Correlating Dirichlet Distributions: Option 2}
Another option for creating similar neighbors is to "mix" nearby neighbors. That is, initialize each state-action transition matrix as a Dirichlet distribution, independently. For every transition vector $w$, with neighbors $\{v_1, v_2, \ldots, v_n\}$, let $\overline{v}$ be the average of the $v_i$. Update $w$ as $w' = rw + (1-r)\overline{v}$, with $r\in[0, 1]$. However, I have not analyzed this in terms of mathematical properties.

\end{document}