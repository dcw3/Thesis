\documentclass{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\EX}{\mathbb{E}}
\DeclareMathOperator{\RE}{\mathbb{RE}}
\newcommand{\ita}{\textit}
\newcommand{\eps}{\epsilon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{\mathbb{P}}

\title{MDP Simulator Proposal}
\author{dcw3}
\date{December 2018}

\begin{document}

\maketitle

\section{Summary of Previous Work}

Overall, I did not find very similar previous work or similar papers. Below are the most relevant previous work I saw:

\subsection{DeepMind Lab}

DeepMind Lab is a 3D RL environment. The user can specify (or programmatically generate) levels, similar to how our simulator will allow users to specify the underlying MDP. The main difference is that DeepMind Lab focuses on a single kind of task (3D navigation), which is fairly complex.
Further research on how level scripts are specified might be useful as inspiration for how we could allow users to programatically specify MDP's.

\subsection{OpenAI Gym}

OpenAI Gym offers a much wider set of tasks than DeepMind Lab, from simple text games to Atari games to robotics tasks. However, most tasks do not offer customization of the environment, and no task allows an arbitrary MDP.

\subsection{Sundry MDP Simulators}

I found two HTML MDP simulators online but they were not particularly useful for this project. One of them was only a Gridworld, and the other only allowed a constant reward for each state, not an arbitrary reward function.

\subsection{TRFL}
Tensorflow supposedly has good RL tools made by Deepmind, in a package called TRFL. I need to look into how to use it, I think.

\subsection{Main Concerns Moving Forward}
How to specify transition matrices and reward functions?

How to write agents? Should I learn Tensorflow? It might be a bad idea to implement algorithms from scratch: supposedly is error-prone.

How to specify priors for agent?

\section{Pseudocode of MDP Simulator}

The pseudocode I have in mind for the MDP simulator is in simulator\_pseudocode.py in this repository.

\section{Defining a Random MDP}
The user specifies $a$, the number of actions, and $m$, the number of states. We then create $a\cdot m$ i.i.d. vectors of length $m$, each of which represents a vector of state-action transition probabilities.

There are two options for choosing these vectors. One is to specify a set number of zero-values for each transition vector, and then choose the remaining values uniformly or logistically. Another option is to choose $m$ values $a_1, \ldots, a_m$ from a standard normal distribution, and then allow the user to specify a cutoff value $\alpha$. Values $a_i$ that are below $\alpha$ are set to $-\infty$. Then, (if any values are remaining) the remaining transition probabilities are set proportional to the value $e^{a_i \beta}$ where $\beta$ is a user-chosen parameter, $\beta\in[0,\infty)$. Higher values of $\beta$ and higher values of $\alpha$ lead to a lower-entropy (more predictable) MDP.

To choose reward functions, the user can specify $m$ values that act as means for normal distributions. The user could also provide $m$ functions as reward functions. We should also provide the option for the user to ask the program to automatically choose reward functions, which could be normal distributions with normally-distributed means.

The starting state will be state $1$ by default.

\section{MDP Generation Process}

Generally speaking, we can view generating an MDP as an iterative process. For now, let us consider a common setting where there is no reward, except for two terminal states, one of which is high-reward and one of which is low-reward.

First, we choose a state $s$ whose transition probabilities are not yet defined. Then, for each action $a$, we choose the states that are reachable from $s$ in one step under action $a$. Then, from within those destination states, we define a transition probability vector with desired entropy. Then, having chosen the transition probability vectors for all actions from $s$, we choose a new state $s$ and begin again.

There is a lot of vagueness and flexibility in this process. The simplest part is choosing a transition probability vector once we have chosen the destination states. If there are $k$ destination states, we use a $k$-dimensional symmetric Dirichlet distribution function, with the concentration parameter $\alpha$. Higher values of $\alpha$ correspond to higher entropy, and this can be specified from the user. For example, for any state-action pair, $\alpha$ could be drawn from a normal distribution with a certain mean.

The next state to choose probabilities for could be chosen from any of the destination states.

The main interesting part would be choosing the destination states of each state-action pair. There are countless properties users might be interested in testing here, so it is hard to account for all possible MDP's the user may wish to create. However, the following are a few properties that come to mind:

\begin{itemize}
  \item How well-structured is the MDP? At the one extreme, all actions from a given state could have the same destination states, just with different probabilities. At the opposite extreme, the destination states of different actions could be completely independent though identically distributed (i.i.d.). The simplest way to specify this would be to specify a distribution for $n_s$ the number of possible destination states, as well as a distribution on $k_{s,a}$ the number of destination states each state-action pair has. Then, each state-action pair's destination states would be uniformly chosen from the $\binom{n_s}{k_{s,a}}$ possible sets of destination states.
  \item How frequently do the states feed into the terminal states? The simplest way to specify this would be to specify some parameter $p$ that grew as the number of states we have yet to specify transition probability vectors for dwindles. $p$ could be the probability that a state is connected to a terminal state.
  \item How recurrent is the MDP? This could be specified as a fraction of the destination states that feed back to a state whose transition probability vectors we have already defined. However, this parameter interacts with the frequency of feeding into terminal states, and I'm not 100\% clear on that yet.
\end{itemize}

\end{document}